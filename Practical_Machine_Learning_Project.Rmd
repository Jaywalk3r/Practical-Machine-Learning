---
title: "Practical Machine Learning Project"
output: html_document
---

###Overview###

The purpose of this project is to use machine learning methods to make accurate predictions of a multiclass response variable using multiple numeric independent variables.

The data for this project were obtained from "six young health participants [who] were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." See [study Web page](http://groupware.les.inf.puc-rio.br/har) for more information about the original study.

###Preliminaries###

We need to load several R packages and download our data.
```{r, echo = FALSE}
setwd( "~/Documents/School/Coursera/Data Science Specialization/Practical Machine Learning/Project")
```

```{r, message = FALSE}
library( caret)

library( mda)

library( kernlab)

library( adabag)

library( rpart)

library( doMC)

registerDoMC( cores = 2)



# set desired working directory here using setwd().

filePath1 = "./pml-training.csv"

filePath2 = "./pml-testing.csv"

if (! file.exists( filePath1)) {

	fileUrl1 = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
	
	download.file( fileUrl1, filePath1, method = "curl")
	
}

if (! file.exists( filePath1)) {

	fileUrl2 = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
	
	download.file( fileUrl2, filePath2, method = "curl")
	
}
```
We open the pml-training.csv file in a text editor to ascertain how missing data are represented. It appears that "NA" strings are used. Additionally, we see "#DIV/0!" strings, presumably indicating attempts to divide by zero.
```{r}
training.raw = read.csv( filePath1, na.strings = c( NA, "#DIV/0!"))

checking.raw = read.csv( filePath2, na.strings = c( NA, "#DIV/0!"))
```

###Data Cleaning###

We first look at the structure of our data.

```{r}
  # truncating to twelve features for space considerations
str( training.raw, list.len = 10)


range( training.raw$X)
```

The first feature appears to replicate row numbers unnecessarily. The following six features do not appear to be useful for making predictions for future data. Some of them, such as the time stamp variables, might be considered important by our prediction models, which is undesirable. We identify these features for non-inclusion in our tidy data.

```{r}
del.index = 1:7
	# initialize a vector containing column indices of predictors in which we
	#	have no interest

```

We also identify all features that contain 60 percent or more NA values.

```{r}
na.proportion = rep( 0 , 159)

for (i in 8:159) {

	na.proportion[ i] = mean( is.na( training.raw[ , i]))

	if (na.proportion[ i] > .6) {
	
		del.index = c( del.index, i)
	
	}

} # add any variable with greater than 60% NA values

# check to see how many features have some, but less than 60 percent NA values
length( na.proportion[ na.proportion > 0 & na.proportion < .6])

```

We also check to see how many remaining features have near zero variance.

```{r}
length( nearZeroVar( training.raw[ , -c( del.index, 160)]))
```

We create new data frames omitting the features we've identified for deletion.

```{r}
training = training.raw[ , -del.index]

checking = checking.raw[ , -del.index]

# verify all observations are complete
sum( ! complete.cases( training))
```

The testing data frame includes only 20 observations. We desire a larger set for testing our models. We partition the training set into a training set and a test set.
```{r}
set.seed( 1123)

inTrain = createDataPartition( training$classe, p = .75, list = FALSE)

testSet = training[ -inTrain,]

training = training[ inTrain,]

```

We may want to ensemble models. We partition the test set into a test set and a validation set, so that a model ensemble can be trained on the test set.

```{r}
set.seed( 11235)

inTest = createDataPartition( testSet$classe, p = .5, list = FALSE)

testing = testSet[ inTest,]

validation = testSet[ -inTest,]

```

###Model Training###
We us ten fold cross validation, repeated five times to train our candidate models.
```{r}
ctrl = trainControl( method = "repeatedcv", number = 10, repeats = 5, savePredictions = TRUE)
```

We train a variety of model types appropriate to multiclass problems, including Mixture Discriminant Analysis, Partial Least Squares Regression, CART, Bagged CART, Support Vector Machines, Random Forests, *K*-Nearest Neighbors, and AdaBoost.M1. Some model types were trained multiple times, using different preprocessing procedures or tuning parameter values. For brevity, we show here the most accurate of each model type.

```{r, eval = FALSE}
# Mixture Discriminant Analysis

set.seed( 112358)

mda2Fit = train( training[ , -53], training$classe,
				method = "mda",
				preProcess = c( "BoxCox", "center", "scale"),
				trControl = ctrl,
				tuneLength = 10)


# K-Nearest Neighbors

# Need to test for k < 5
knnGrid = data.frame( k = 1:10)


set.seed( 112358)

knn3Fit = train( training[, -53], training$classe,
				 method = "knn",
				 preProcess = c( "center", "scale"),
				 trControl = ctrl,
				 tuneLength = 10,
				 tuneGrid = knnGrid)


# Partial Least Squares

set.seed( 112358)

pls2Fit = train( training[, -53], training$classe,
				 method = "pls",
				 preProcess = c( "BoxCox", "center", "scale"),
				 trControl = ctrl,
				 tuneLength = 10,
				 tuneGrid = expand.grid( .ncomp = 42:52))


# Random Forest

rfGrid = data.frame( mtry = 3:12)

set.seed( 112358)

rf1Fit = train( training[, -53], training$classe,
			    method = "rf",
			    preProcess = c( "center", "scale"),
			    trControl = ctrl,
			    tuneGrid = rfGrid)


# Boosting (AdaBoost.M1)

adaboostGrid = data.frame( coeflearn = rep( "Zhu", 6), maxdepth = rep( 3, 6), mfinal = 100 * 5:10)

set.seed( 112358)

adaboost2Fit = train( training[, -53], training$classe,
					 method = "AdaBoost.M1",
					 preProcess = c( "center", "scale"),
					 trControl = ctrl,
					 tuneGrid = adaboostGrid)


# CART

set.seed( 112358)

cartFit = train( training[, -53], training$classe,
				 method = "rpart",
				 preProcess = c( "center", "scale"),
				 trControl = ctrl,
				 tuneLength = 10)


# Bagged CART

set.seed( 112358)

bagcartFit = train( training[, -53], training$classe,
				 method = "rpart",
				 preProcess = c( "center", "scale"),
				 trControl = ctrl)


# Support Vector Machines

sigmaRange = sigest( as.matrix( training[ , -53])) # sigma estimate

svmRGrid2 = expand.grid( .sigma = sigmaRange[ 1], .C = c( 20, 24, 32, 36))
								   
set.seed( 112358)

svm2Fit = train( training[, -53], training$classe,
				 method = "svmRadial",
				 preProcess = c( "center", "scale"),
				 trControl = ctrl,
				 tuneGrid = svmRGrid2)
```

```{r, echo = FALSE}
mda2Fit = readRDS( "./mda2Fit.rds")

knn3Fit = readRDS( "./knn3Fit.rds")

pls2Fit = readRDS( "./pls2Fit.rds")

rf1Fit = readRDS( "./rf1Fit.rds")

adaboost2Fit = readRDS( "./adaboost2Fit.rds")

cartFit = readRDS( "./cartFit.rds")

bagcartFit = readRDS( "./bagcartFit.rds")

svm2Fit = readRDS( "./svm2Fit.rds")
```

We check each model's best accuracy, based on the training data.

```{r}
# Mixture Discriminant Analysis
max( mda2Fit$results$Accuracy)

# K-Nearest Neighbors
max( knn3Fit$results$Accuracy)

# Partial Least Squares
max( pls2Fit$results$Accuracy)

# Random Forest
max( rf1Fit$results$Accuracy)

# AdaBoost.M1
max( adaboost2Fit$results$Accuracy)

# CART
max( cartFit$results$Accuracy)

# Bagged CART
max( bagcartFit$results$Accuracy)

# Support Vector Machines
max( svm2Fit$results$Accuracy)
```


We see that the accuracy estimates for the *K)-Nearest Neighbors, Random Forest, AdaBoost.M1, and Support Vector Machines models all exceed 97 percent. We test the performance of these three models with the test set.

```{r, message = FALSE}
predictionKNN3 = predict( knn3Fit, testing[ ,-53])
				 
(knn3Results = confusionMatrix( predictionKNN3, testing$classe))


predictionRF1 = predict( rf1Fit, testing[ , -53])

(rf1Results = confusionMatrix( predictionRF1, testing$classe))


predictionAdaBoost = predict( adaboost2Fit, testing[ , -53])

(adaboost2Results = confusionMatrix( predictionAdaBoost, testing$classe))


predictionSVM2 = predict( svm2Fit, testing[ , -53])

(svm2Results = confusionMatrix( predictionSVM2, testing$classe))
```

We see that the predicted accuracy rates based on the training data fall within the 95 percent confidence intervals for accuracy, calculated from the test data, suggesting our choice of repeated cross-validation for our resampling was good.

We look more closely at the confidence intervals to see if there is a statistically significant difference between the performances of the models.

```{r}
accuracy = rbind( knn3Results$overall[ c( 3, 1, 4)],
                  rf1Results$overall[ c( 3, 1, 4)],
                  adaboost2Results$overall[ c( 3, 1, 4)],
                  svm2Results$overall[ c( 3, 1, 4)])

rownames( accuracy) = c("KNN", "RF", "AdaBoost", "SVM")

colnames( accuracy) = c( "Lower", "Accuracy", "Upper")

accuracy
```


###Ensembling###

The intervals for *K*-Nearest Neighbors and Random Forest models overlap, and the intervals for *K*-Nearest Neighbors and Support Vector Machines models overlap, but the upper bound of the confidence interval for the AdaBoost.M1 model is lower than the lower bound of the Support Vector Machines confidence interval. From this, we hypothesize that there is a statistically significant difference in the accuracy rate of the AdaBoost.M1 model compared to the other three models.

We test this hypothesis with R's *prop.test()* function, which has as a null hypothesis that proportions (accuracy rates, in this case) are equal.
```{r}
n = length( testing$classe)

prop.test( n * accuracy[ , 1], rep( n, 4))$p.value
```

We eliminate the AdaBoost.M1 model from consideration for ensembling.

We don't know that there is not a statistically significant difference in the accuracy rates of the remaining models, but we need at least three models in order for a voting ensemble to be useful.

We try three different ensembles, weighted voting, AdaBoost.M1, and CART.

For voting, we elect to use weighted voting to avoid ties. The weight of a model's vote is its positive predictive value for the class it has predicted, in other words, the probability that Class = *k* given the model predicted class *k*.

We begin by creating a matrix of positive predictive values.

```{r}
PPV = cbind( svm2Results$byClass[ , "Pos Pred Value"],
             knn3Results$byClass[ , "Pos Pred Value"],
             rf1Results$byClass[ , "Pos Pred Value"])

colnames( PPV) = c( "SVM", "KNN", "RF")

rownames( PPV) = c( "A", "B", "C", "D", "E")
```
To avoid the case of a tie, we verify that any matching positive predictive values are associated with predictions of the same class. In other words, we verify the positive predictive value for one model is equal to the positive predictive value of another model only if both models make the same prediction.

```{r}
count = 0

for (i in 1:4) {

	for (j in (i + 1):5) {
	
		count = count + sum( PPV[ j,] %in% PPV[ i,])
	
	}

}

count
```

We write a function to determine the prediction for a single observation.
```{r}
prediction = function( council) {
		# council is vector of predictions for single observation

	if (length( unique( council)) == 1) {
			# all models agree on predicted class
			
		return( council[ 1])
	
	} else {
			# disagreement among models regarding predicted class results in
			#	weighted vote
			
		ballotBox = data.frame( candidate = c( "A", "B", "C", "D", "E"), votes = rep( 0, 5))
		
		k = length( council)
		
		for( i in 1:k) {
		
			ballotBox$votes[ ballotBox$candidate == council[ i]] = ballotBox$votes[ ballotBox$candidate
					== council[ i]] + PPV[ council[ i], i]
				# each model's vote weight/value is equal to model's positive
				#	predictive value for the class it predicts.
			
		}
		
		returnIndex = max( ballotBox$votes) == ballotBox$votes
			# determine which class received most votes
			
			if (sum( returnIndex == 1)) {
				
				return( as.character( ballotBox$candidate[ returnIndex]))
					# if as.character() is not used, then value is returned as
					#	integer.
				
			}
	
	}

}
```

We assemble a matrix of predictions of each our three best individual models, with each column representing a model. We also check the accuracy rate of our voting ensemble, since it was not built using the testing data. To obtain predictions for it, we pass our *prediction()* function to *apply()*

```{r}
modelList = list( SVM = svm2Fit, KNN = knn3Fit, RF = rf1Fit)

allPredictions = sapply( modelList, predict, newdata = testing[ , -53])

predictionVote = as.factor( apply( allPredictions, 1, prediction))
  # get predictions based on individual models voting.

confusionMatrix( predictionVote, testing$classe)
```

We train the other two ensembles using the training package.

```{r, eval = FALSE}
set.seed( 112358)

ensemble2 = train( allPredictions, testing$classe,
				   method = "AdaBoost.M1",
				   trControl = ctrl)

set.seed( 112358)

ensemble3 = train( allPredictions, testing$classe,
				   method = "rpart",
				   trControl = ctrl,
				   tuneLength = 10)
```

```{r, echo = FALSE}
ensemble2 = readRDS( "ensemble2.rds")

ensemble3 = readRDS( "ensemble3.rds")
```
We pass the validation data set to each of our three individual models simultaneously, creating a matrix of predictions. We also obtain the predicted classes from the ensembles.

```{r}
predMatrix = sapply( modelList, predict, newdata = validation[ , -53])

validationVote = as.factor( apply( predMatrix, 1, prediction))

validationEn2pred = predict( ensemble2, predMatrix)

validationEn3pred = predict( ensemble3, predMatrix)
```
We calculate the confusion matrices for the individual models and the ensembles.
```{r}
svmConfusion = confusionMatrix( predMatrix[ , 1], validation$classe)

knnConfusion = confusionMatrix( predMatrix[ , 2], validation$classe)

rfConfusion = confusionMatrix( predMatrix[ , 3], validation$classe)

voteConfusion = confusionMatrix( validationVote, validation$classe)

en2Confusion = confusionMatrix( validationEn2pred, validation$classe)

en3Confusion = confusionMatrix( validationEn3pred, validation$classe)
```
For ease of comparison, we create a matrix with accuracy estimate, kappa estimate, and 95 percent accuracy confidence interval.
```{r}
AccCompMat = rbind( svmConfusion$overall[c( 3, 1, 4, 2)],
					knnConfusion$overall[c( 3, 1, 4, 2)],
					rfConfusion$overall[c( 3, 1, 4, 2)],
					voteConfusion$overall[c( 3, 1, 4, 2)],
					en2Confusion$overall[c( 3, 1, 4, 2)],
					en3Confusion$overall[c( 3, 1, 4, 2)])
					
rownames( AccCompMat) = c( "SVM", "KNN", "RF", "Vote", "EnsAB", "EnsCART")

AccCompMat
```
We check to see if there is a statistically significant difference among any of the accuracy measurements.
```{r}
n = length( validation$classe)

prop.test( n * AccCompMat[ , 2], rep( n, 6))$p.value
```
We drop the Support Vector Machines model from consideration and recheck.
```{r}
prop.test( n * AccCompMat[ 2:6, 2], rep( n, 5))$p.value
```
The difference is no longer statistically significant. However, there's less than a fifteen percent chance that we'd see results as extreme as we're seeing if the null hypothesis (that all the accuracy rates are equal) is true. That's not very reassuring. We drop the *K*-Nearest Neighbors model from consideration.
```{r}
prop.test( n * AccCompMat[ 3:6, 2], rep( n, 4))$p.value
```
Now the p-value is close to 0.5. If the null hypothesis is true, we expect to see estimates with differences at least as extreme as we currently see about half of the time.

We are content to use the Random Forest model or any of the ensemble models.

###Final Prediction###

All that's left is to predict the twenty classes of the checking data set.

```{r}
checkPredMatrix = sapply( modelList, predict, newdata = checking[ , -53])

checkingVote = as.factor( apply( checkPredMatrix, 1, prediction))

checkingEn2pred = predict( ensemble2, checkPredMatrix)

checkingEn3pred = predict( ensemble3, checkPredMatrix)

checkPredMatrix

identical( checkingVote, checkingEn2pred, checkingEn3pred)

```
While we are only interested in the Random Forest model, all of the individual models agree with their predictions. By design, the voting ensemble's predictions match those. The predictions of the other two ensembles also match, as expected.

###Software###
```{r}
sessionInfo()
```

